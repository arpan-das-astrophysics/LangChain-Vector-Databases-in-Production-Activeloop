{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNbTEkkriVPmDcGJR9eG0q8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arpan-das-astrophysics/LangChain-Vector-Databases-in-Production-Activeloop/blob/main/What_are_Text_Splitters_and_Why_They_are_Useful.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **What are Text Splitters and Why They are Useful**"
      ],
      "metadata": {
        "id": "g-669bH8l-BG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Introduction**\n",
        "Large Language Models, while recognized for creating human-like text, can also \"hallucinate\" and produce seemingly plausible yet incorrect or nonsensical information. Interestingly, this tendency can be advantageous in creative tasks, as it generates a range of unique and imaginative ideas, sparking new perspectives and driving the creative process. However, this poses a challenge in situations where accuracy is critical, such as code reviews, insurance-related tasks, or research question responses.\n",
        "\n",
        "One approach to mitigating hallucination is to provide documents as sources of information to the LLM and ask it to generate an answer based on the knowledge extracted from the document. This can help reduce the likelihood of hallucination, and users can verify the information with the source document.\n",
        "\n",
        "Let's discuss the pros and cons of this approach:\n",
        "\n",
        "### **Pros:**\n",
        "\n",
        "1. **Reduced hallucination:** By providing a source document, the LLM is more likely to generate content based on the given information, reducing the chances of creating false or irrelevant information.\n",
        "2. **Increased accuracy:** With a reliable source document, the LLM can generate more accurate answers, especially in use cases where accuracy is crucial.\n",
        "3. **Verifiable information:** Users can cross-check the generated content with the source document to ensure the information is accurate and reliable.\n",
        "\n",
        "### **Cons:**\n",
        "\n",
        "1. **Limited scope:** Relying on a single document may limit the scope of the generated content, as the LLM will only have access to the information provided in the document.\n",
        "2. **Dependence on document quality:** The accuracy of the generated content heavily depends on the quality and reliability of the source document. The LLM will likely generate incorrect or misleading content if the document contains inaccurate or biased information.\n",
        "3. **Inability to eliminate hallucination completely:** Although providing a document as a base reduces the chances of hallucination, it does not guarantee that the LLM will never generate false or irrelevant information.\n",
        "\n",
        "Addressing another challenge, LLMs have a maximum prompt size, preventing them from feeding entire documents. This makes it crucial to divide documents into smaller parts, and Text Splitters prove to be extremely useful in achieving this. Text Splitters help break down large text documents into smaller, more digestible pieces that language models can process more effectively.\n",
        "\n",
        "Using a Text Splitter can also improve vector store search results, as smaller segments might be more likely to match a query. Experimenting with different chunk sizes and overlaps can be beneficial in tailoring results to suit your specific needs.\n"
      ],
      "metadata": {
        "id": "zeqql6mCmB6V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Customizing Text Splitter**\n",
        "When handling lengthy pieces of text, it's crucial to break them down into manageable chunks. This seemingly simple task can quickly become complex, as keeping semantically related text segments intact is essential. The definition of \"semantically related\" may vary depending on the type of text. In this article, we'll explore various strategies to achieve this.\n",
        "\n",
        "At a high level, text splitters follow these steps:\n",
        "\n",
        "1. Divide the text into small, semantically meaningful chunks (often sentences).\n",
        "2. Combine these small chunks into a larger one until a specific size is reached (determined by a particular function).\n",
        "3. Once the desired size is attained, separate that chunk as an individual piece of text, then start forming a new chunk with some overlap to maintain context between segments.\n",
        "\n",
        "Consequently, there are two primary dimensions to consider when customizing your text splitter:\n",
        "\n",
        "* The method used to split the text\n",
        "* The approach for measuring chunk size"
      ],
      "metadata": {
        "id": "VaP_DIhRoBow"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Character Text Splitter**\n",
        "This type of splitter can be used in various scenarios where you must split long text pieces into smaller, semantically meaningful chunks. For example, you might use it to split a long article into smaller chunks for easier processing or analysis. The splitter allows you to customize the chunking process along two axes - chunk size and chunk overlap - to balance the trade-offs between splitting the text into manageable pieces and preserving semantic context between chunks."
      ],
      "metadata": {
        "id": "ffov4BBqobMv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3ehekNTPk2R4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6b252d4-5f92-4c38-86d6-6c101a0c70a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m255.0/255.0 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q langchain==0.0.208 tiktoken pypdf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PyPDFLoader\n",
        "loader = PyPDFLoader(\"The One Page Linux Manual.pdf\")\n",
        "pages = loader.load_and_split()"
      ],
      "metadata": {
        "id": "6kKv8aOZorHZ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "By loading the text file, we can ask more specific questions related to the subject, which helps minimize the likelihood of LLM hallucinations and ensures more accurate, context-driven responses."
      ],
      "metadata": {
        "id": "n_DPGm50pWy0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
        "texts = text_splitter.split_documents(pages)\n",
        "\n",
        "print(texts[0])\n",
        "\n",
        "print (f\"You have {len(texts)} documents\")\n",
        "\n",
        "print (\"Preview:\")\n",
        "print (texts[0].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afpY4D8ro3ep",
        "outputId": "c4b91f47-3653-4ca4-b767-f6c94423ada1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "page_content='THE ONE     PAGE LINUX MANUALA summary of useful Linux commands\\nVersion 3.0 May 1999 squadron@powerup.com.au\\nStarting & Stopping\\nshutdown -h now Shutdown the system now and do not\\nreboot\\nhalt Stop all processes - same as above\\nshutdown -r 5 Shutdown the system in 5 minutes and\\nreboot\\nshutdown -r now Shutdown the system now and reboot\\nreboot Stop all processes and then reboot - same\\nas above\\nstartx Start the X system\\nAccessing & mounting file systems\\nmount -t iso9660 /dev/cdrom\\n/mnt/cdromMount the device cdrom\\nand call it cdrom under the\\n/mnt directory\\nmount -t msdos /dev/hdd\\n/mnt/ddriveMount hard disk “d” as a\\nmsdos file system and call\\nit ddrive under the /mnt\\ndirectory\\nmount -t vfat /dev/hda1\\n/mnt/cdriveMount hard disk “a” as a\\nVFAT file system and call it\\ncdrive under the /mnt\\ndirectory\\numount /mnt/cdrom Unmount the cdrom\\nFinding files and text within files\\nfind / -name  fname Starting with the root directory, look\\nfor the file called fname\\nfind / -name ”*fname* ” Starting with the root directory, look\\nfor the file containing the string fname\\nlocate missingfilename Find a file called missingfilename\\nusing the locate command - this\\nassumes you have already used the\\ncommand updatedb (see next)\\nupdatedb Create or update the database of files\\non all file systems attached to the linux\\nroot directory\\nwhich missingfilename Show the subdirectory containing the\\nexecutable file  called missingfilename\\ngrep textstringtofind\\n/dirStarting with the directory called dir ,\\nlook for and list all files containing\\ntextstringtofind\\nThe X Window System\\nxvidtune Run the X graphics tuning utility\\nXF86Setup Run the X configuration menu with\\nautomatic probing of graphics cards\\nXconfigurator Run another X configuration menu with\\nautomatic probing of graphics cards\\nxf86config Run a text based X configuration menu\\nMoving, copying, deleting & viewing files\\nls -l List files in current directory using\\nlong format\\nls -F List files in current directory and\\nindicate the file type\\nls -laC List all files in current directory in\\nlong format and display in columnsrm name Remove a file or directory called\\nname\\nrm -rf name Kill off an entire directory and all it’s\\nincludes files and subdirectories\\ncp filename\\n/home/dirnameCopy the file called filename to the\\n/home/dirname directory\\nmv filename\\n/home/dirnameMove the file called filename to the\\n/home/dirname directory\\ncat filetoview Display the file called filetoview\\nman -k keyword Display man pages containing\\nkeyword\\nmore filetoview Display the file called filetoview one\\npage at a time, proceed to next page\\nusing the spacebar\\nhead filetoview Display the first 10 lines of the file\\ncalled filetoview\\nhead -20 filetoview Display the first 20 lines of the file\\ncalled filetoview\\ntail filetoview Display the last 10 lines of the file\\ncalled filetoview\\ntail -20 filetoview Display the last 20 lines of the file\\ncalled filetoview\\nInstalling software for Linux\\nrpm -ihv name.rpm Install the rpm package called name\\nrpm -Uhv name.rpm Upgrade the rpm package called\\nname\\nrpm -e package Delete the rpm package called\\npackage\\nrpm -l package List the files in the package called\\npackage\\nrpm -ql package List the files and state the installed\\nversion of the package called\\npackage\\nrpm -i --force package Reinstall the rpm package called\\nname having deleted parts of it (not\\ndeleting using rpm -e)\\ntar -zxvf archive.tar.gz or\\ntar -zxvf archive.tgzDecompress the files contained in\\nthe zipped and tarred archive called\\narchive\\n./configure Execute the script preparing the\\ninstalled files for compiling\\nUser Administration\\nadduser accountname Create a new user call accountname\\npasswd accountname Give accountname a new password\\nsu Log in as superuser from current login\\nexit Stop being superuser and revert to\\nnormal user\\nLittle known tips and tricks\\nifconfig List ip addresses for all devices on\\nthe machine\\napropos subject List manual pages for subject\\nusermount Executes graphical application for\\nmounting and unmounting file\\nsystems' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
            "You have 2 documents\n",
            "Preview:\n",
            "THE ONE     PAGE LINUX MANUALA summary of useful Linux commands\n",
            "Version 3.0 May 1999 squadron@powerup.com.au\n",
            "Starting & Stopping\n",
            "shutdown -h now Shutdown the system now and do not\n",
            "reboot\n",
            "halt Stop all processes - same as above\n",
            "shutdown -r 5 Shutdown the system in 5 minutes and\n",
            "reboot\n",
            "shutdown -r now Shutdown the system now and reboot\n",
            "reboot Stop all processes and then reboot - same\n",
            "as above\n",
            "startx Start the X system\n",
            "Accessing & mounting file systems\n",
            "mount -t iso9660 /dev/cdrom\n",
            "/mnt/cdromMount the device cdrom\n",
            "and call it cdrom under the\n",
            "/mnt directory\n",
            "mount -t msdos /dev/hdd\n",
            "/mnt/ddriveMount hard disk “d” as a\n",
            "msdos file system and call\n",
            "it ddrive under the /mnt\n",
            "directory\n",
            "mount -t vfat /dev/hda1\n",
            "/mnt/cdriveMount hard disk “a” as a\n",
            "VFAT file system and call it\n",
            "cdrive under the /mnt\n",
            "directory\n",
            "umount /mnt/cdrom Unmount the cdrom\n",
            "Finding files and text within files\n",
            "find / -name  fname Starting with the root directory, look\n",
            "for the file called fname\n",
            "find / -name ”*fname* ” Starting with the root directory, look\n",
            "for the file containing the string fname\n",
            "locate missingfilename Find a file called missingfilename\n",
            "using the locate command - this\n",
            "assumes you have already used the\n",
            "command updatedb (see next)\n",
            "updatedb Create or update the database of files\n",
            "on all file systems attached to the linux\n",
            "root directory\n",
            "which missingfilename Show the subdirectory containing the\n",
            "executable file  called missingfilename\n",
            "grep textstringtofind\n",
            "/dirStarting with the directory called dir ,\n",
            "look for and list all files containing\n",
            "textstringtofind\n",
            "The X Window System\n",
            "xvidtune Run the X graphics tuning utility\n",
            "XF86Setup Run the X configuration menu with\n",
            "automatic probing of graphics cards\n",
            "Xconfigurator Run another X configuration menu with\n",
            "automatic probing of graphics cards\n",
            "xf86config Run a text based X configuration menu\n",
            "Moving, copying, deleting & viewing files\n",
            "ls -l List files in current directory using\n",
            "long format\n",
            "ls -F List files in current directory and\n",
            "indicate the file type\n",
            "ls -laC List all files in current directory in\n",
            "long format and display in columnsrm name Remove a file or directory called\n",
            "name\n",
            "rm -rf name Kill off an entire directory and all it’s\n",
            "includes files and subdirectories\n",
            "cp filename\n",
            "/home/dirnameCopy the file called filename to the\n",
            "/home/dirname directory\n",
            "mv filename\n",
            "/home/dirnameMove the file called filename to the\n",
            "/home/dirname directory\n",
            "cat filetoview Display the file called filetoview\n",
            "man -k keyword Display man pages containing\n",
            "keyword\n",
            "more filetoview Display the file called filetoview one\n",
            "page at a time, proceed to next page\n",
            "using the spacebar\n",
            "head filetoview Display the first 10 lines of the file\n",
            "called filetoview\n",
            "head -20 filetoview Display the first 20 lines of the file\n",
            "called filetoview\n",
            "tail filetoview Display the last 10 lines of the file\n",
            "called filetoview\n",
            "tail -20 filetoview Display the last 20 lines of the file\n",
            "called filetoview\n",
            "Installing software for Linux\n",
            "rpm -ihv name.rpm Install the rpm package called name\n",
            "rpm -Uhv name.rpm Upgrade the rpm package called\n",
            "name\n",
            "rpm -e package Delete the rpm package called\n",
            "package\n",
            "rpm -l package List the files in the package called\n",
            "package\n",
            "rpm -ql package List the files and state the installed\n",
            "version of the package called\n",
            "package\n",
            "rpm -i --force package Reinstall the rpm package called\n",
            "name having deleted parts of it (not\n",
            "deleting using rpm -e)\n",
            "tar -zxvf archive.tar.gz or\n",
            "tar -zxvf archive.tgzDecompress the files contained in\n",
            "the zipped and tarred archive called\n",
            "archive\n",
            "./configure Execute the script preparing the\n",
            "installed files for compiling\n",
            "User Administration\n",
            "adduser accountname Create a new user call accountname\n",
            "passwd accountname Give accountname a new password\n",
            "su Log in as superuser from current login\n",
            "exit Stop being superuser and revert to\n",
            "normal user\n",
            "Little known tips and tricks\n",
            "ifconfig List ip addresses for all devices on\n",
            "the machine\n",
            "apropos subject List manual pages for subject\n",
            "usermount Executes graphical application for\n",
            "mounting and unmounting file\n",
            "systems\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "No universal approach for chunking text will fit all scenarios - what's effective for one case might not be suitable for another. Finding the best chunk size for your project means going through a few steps. First, clean up your data by getting rid of anything that's not needed, like HTML tags from websites. Then, pick a few different chunk sizes to test. The best size will depend on what kind of data you're working with and the model you're using.  Finally, test out how well each size works by running some queries and comparing the results. You might need to try a few different sizes before finding the best one. This process might take some time, but getting the best results from your project is worth it."
      ],
      "metadata": {
        "id": "CPOOrcC0pich"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Recursive Character Text Splitter**\n",
        "The Recursive Character Text Splitter is a text splitter designed to split the text into chunks based on a list of characters provided. It attempts to split text using the characters from a list in order until the resulting chunks are small enough. By default, the list of characters used for splitting is [\"\\n\\n\", \"\\n\", \" \", \"], which tries to keep paragraphs, sentences, and words together as long as possible, as they are generally the most semantically related pieces of text. This means that the class first tries to split the text into two new-line characters. If the resulting chunks are still larger than the desired chunk size, it will then try to split the output by a single new-line character, followed by a space character, and so on, until the desired chunk size is achieved.\n",
        "\n",
        "To use the RecursiveCharacterTextSplitter, you can create an instance of it and provide the following parameters:\n",
        "\n",
        "`chunk_size :` The maximum size of the chunks, as measured by the length_function (default is 100).\n",
        "\n",
        "`chunk_overlap:` The maximum overlap between chunks to maintain continuity between them (default is 20).\n",
        "\n",
        "`length_function:` parameter is used to calculate the length of the chunks. By default, it is set to len, which counts the number of characters in a chunk. However, you can also pass a token counter or any other function that calculates the length of a chunk based on your specific requirements.\n",
        "\n",
        "Using a token counter instead of the default len function can benefit specific scenarios, such as when working with language models with token limits. For example, OpenAI's GPT-3 has a token limit of 4096 tokens per request, so you might want to count tokens instead of characters to better manage and optimize your requests.\n",
        "\n",
        "Here's an example of how to use RecursiveCharacterTextSplitter"
      ],
      "metadata": {
        "id": "bnrswHgWp19Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!echo \"Helllo, my name is Ala\\n Hello again\\n\\ntesting newline.\" > LLM.txt"
      ],
      "metadata": {
        "id": "ogDuf9N1pZIt"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Load a long document\n",
        "with open('LLM.txt', encoding= 'unicode_escape') as f:\n",
        "    sample_text = f.read()"
      ],
      "metadata": {
        "id": "aDXx83Ftsj5e"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create an instance of the RecursiveCharacterTextSplitter class with the desired parameters. The default list of characters to split by is [\"\\n\\n\", \"\\n\", \" \", \"\"]."
      ],
      "metadata": {
        "id": "vjkk5PDnsvLg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=50,\n",
        "    chunk_overlap=10,\n",
        "    length_function=len,\n",
        ")"
      ],
      "metadata": {
        "id": "QbDDNsAdssIw"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The text is first split by two new-line characters (\\n\\n). Then, since the chunks are still larger than the desired chunk size (50), the class tries to split the output by a single new-line character (\\n). The final output consists of two chunks, each with a length of 50 characters or less."
      ],
      "metadata": {
        "id": "b4vWBE8as0gh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texts = text_splitter.create_documents([sample_text])\n",
        "print(texts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L1wFnSTxsx-O",
        "outputId": "a146eb4b-bb2d-4531-e184-915b91156507"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Document(page_content='Helllo, my name is Ala\\n Hello again', metadata={}), Document(page_content='testing newline.', metadata={})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, the text is loaded from a file, and the RecursiveCharacterTextSplitter is used to split it into chunks with a maximum size of 50 characters and an overlap of 10 characters. The output will be a list of documents containing the split text.\n",
        "\n",
        "To use a token counter, you can create a custom function that calculates the number of tokens in a given text and pass it as the length_function parameter. This will ensure that your text splitter calculates the length of chunks based on the number of tokens instead of the number of characters. The exploration of this concept will be part of our upcoming lessons."
      ],
      "metadata": {
        "id": "M8TQ5HTrs-uK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **NLTK Text Splitter**\n",
        "The NLTKTextSplitter in LangChain is an implementation of a text splitter that uses the Natural Language Toolkit (NLTK) library to split text based on tokenizers. The goal is to split long texts into smaller chunks without breaking the structure of sentences and paragraphs."
      ],
      "metadata": {
        "id": "B9TLbBidwwJQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q nltk"
      ],
      "metadata": {
        "id": "mAmamkQLs4zY"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r31E_XzGxj7T",
        "outputId": "fe8d75c9-9137-4031-f487-4e990e7bd679"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a long document\n",
        "with open('LLM.txt', encoding= 'unicode_escape') as f:\n",
        "    sample_text = f.read()\n",
        "\n",
        "from langchain.text_splitter import NLTKTextSplitter\n",
        "text_splitter = NLTKTextSplitter(chunk_size=500)\n",
        "\n",
        "\n",
        "texts = text_splitter.split_text(sample_text)\n",
        "print(texts)\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lhXJMwhtxvdw",
        "outputId": "55e7b6b2-3816-49b2-eb4f-ed9d7ebd2bf3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Helllo, my name is Ala\\n Hello again\\n\\ntesting newline.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "However, as mentioned in your context, the NLTKTextSplitter is not specifically designed to handle word segmentation in English sentences without spaces. For this purpose, you can use alternative libraries like pyenchant or word segment."
      ],
      "metadata": {
        "id": "W0vFRvs7x8ZZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **SpacyTextSplitter**\n",
        "The `SpacyTextSplitter` helps split large text documents into smaller chunks based on a specified size. This is useful for better management of large text inputs. It's important to note that the `SpacyTextSplitter` is an alternative to NLTK-based sentence splitting. You can create a `SpacyTextSplitter` object by specifying the chunk_size parameter, measured by a length function passed to it, which defaults to the number of characters."
      ],
      "metadata": {
        "id": "TKk8E8WdyB8W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import SpacyTextSplitter\n",
        "\n",
        "\n",
        "# Load a long document\n",
        "with open('LLM.txt', encoding= 'unicode_escape') as f:\n",
        "    sample_text = f.read()\n",
        "\n",
        "# Instantiate the SpacyTextSplitter with the desired chunk size\n",
        "text_splitter = SpacyTextSplitter(chunk_size=500, chunk_overlap=20)\n",
        "\n",
        "\n",
        "# Split the text using SpacyTextSplitter\n",
        "texts = text_splitter.split_text(sample_text)\n",
        "\n",
        "# Print the first chunk\n",
        "print(texts)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HyWYkpvhxzuJ",
        "outputId": "a9fa98ec-f5ce-4851-c419-af9090f43fbe"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Helllo, my name is Ala\\n \\n\\nHello again\\n\\ntesting newline.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **MarkdownTextSplitter**\n",
        "The `MarkdownTextSplitter` is designed to split text written using Markdown languages like headers, code blocks, or dividers. It is implemented as a simple subclass of RecursiveCharacterSplitter with Markdown-specific separators. By default, these separators are determined by the Markdown syntax, but they can be customized by providing a list of characters during the initialization of the `MarkdownTextSplitter` instance. The chunk size, which is initially set to the number of characters, is measured by the length function passed in. To customize the chunk size, provide an integer value when initializing an instance.\n",
        "\n"
      ],
      "metadata": {
        "id": "PPnFvOR3yRaC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import MarkdownTextSplitter\n",
        "markdown_text = \"\"\"\n",
        "#\n",
        "\n",
        "# Welcome to My Blog!\n",
        "\n",
        "## Introduction\n",
        "Hello everyone! My name is **John Doe** and I am a _software developer_. I specialize in Python, Java, and JavaScript.\n",
        "\n",
        "Here's a list of my favorite programming languages:\n",
        "\n",
        "1. Python\n",
        "2. JavaScript\n",
        "3. Java\n",
        "\n",
        "You can check out some of my projects on [GitHub](https://github.com).\n",
        "\n",
        "## About this Blog\n",
        "In this blog, I will share my journey as a software developer. I'll post tutorials, my thoughts on the latest technology trends, and occasional book reviews.\n",
        "\n",
        "Here's a small piece of Python code to say hello:\n",
        "\n",
        "\\``` python\n",
        "def say_hello(name):\n",
        "    print(f\"Hello, {name}!\")\n",
        "\n",
        "say_hello(\"John\")\n",
        "\\```\n",
        "\n",
        "Stay tuned for more updates!\n",
        "\n",
        "## Contact Me\n",
        "Feel free to reach out to me on [Twitter](https://twitter.com) or send me an email at johndoe@email.com.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "markdown_splitter = MarkdownTextSplitter(chunk_size=100, chunk_overlap=0)\n",
        "docs = markdown_splitter.create_documents([markdown_text])\n",
        "\n",
        "print(docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VrrVbBGJyNL0",
        "outputId": "634b16bd-a551-448b-cbb3-83a61fb3c75f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Document(page_content='# \\n\\n# Welcome to My Blog!', metadata={}), Document(page_content='## Introduction', metadata={}), Document(page_content='Hello everyone! My name is **John Doe** and I am a _software developer_. I specialize in Python,', metadata={}), Document(page_content='Java, and JavaScript.', metadata={}), Document(page_content=\"Here's a list of my favorite programming languages:\\n\\n1. Python\\n2. JavaScript\\n3. Java\", metadata={}), Document(page_content='You can check out some of my projects on [GitHub](https://github.com).', metadata={}), Document(page_content='## About this Blog', metadata={}), Document(page_content=\"In this blog, I will share my journey as a software developer. I'll post tutorials, my thoughts on\", metadata={}), Document(page_content='the latest technology trends, and occasional book reviews.', metadata={}), Document(page_content=\"Here's a small piece of Python code to say hello:\", metadata={}), Document(page_content='\\\\``` python\\ndef say_hello(name):\\n    print(f\"Hello, {name}!\")\\n\\nsay_hello(\"John\")\\n\\\\', metadata={}), Document(page_content='```\\n\\nStay tuned for more updates!', metadata={}), Document(page_content='## Contact Me', metadata={}), Document(page_content='Feel free to reach out to me on [Twitter](https://twitter.com) or send me an email at', metadata={}), Document(page_content='johndoe@email.com.', metadata={})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The MarkdownTextSplitter offers a practical solution for dividing text while preserving the structure and meaning provided by Markdown formatting. By recognizing the Markdown syntax (e.g., headings, lists, and code blocks), you can intelligently divide the content based on its structure and hierarchy, resulting in more semantically coherent chunks. This splitter is especially valuable when managing extensive Markdown documents."
      ],
      "metadata": {
        "id": "e4e7gUdRyiCa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **TokenTextSplitter**\n",
        "The main advantage of using `TokenTextSplitter` over other text splitters, like `CharacterTextSplitter`, is that it respects the token boundaries, ensuring that the chunks do not split tokens in the middle. This can be particularly helpful in maintaining the semantic integrity of the text when working with language models and embeddings.\n",
        "\n",
        "This type of splitter breaks down raw text strings into smaller pieces by initially converting the text into BPE (Byte Pair Encoding) tokens, and subsequently dividing these tokens into chunks. It then reassembles the tokens within each chunk back into text."
      ],
      "metadata": {
        "id": "qu7vMYIpyvhG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from langchain.text_splitter import TokenTextSplitter\n",
        "\n",
        "# Load a long document\n",
        "with open('LLM.txt', encoding= 'unicode_escape') as f:\n",
        "    sample_text = f.read()\n",
        "\n",
        "# Initialize the TokenTextSplitter with desired chunk size and overlap\n",
        "text_splitter = TokenTextSplitter(chunk_size=100, chunk_overlap=50)\n",
        "\n",
        "# Split into smaller chunks\n",
        "texts = text_splitter.split_text(sample_text)\n",
        "print(texts[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xpNtbhFyye8Y",
        "outputId": "8793b8e9-72e4-44f1-e132-1a15d1cb45d5"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Helllo, my name is Ala\n",
            " Hello again\n",
            "\n",
            "testing newline.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chunk_size parameter sets the maximum number of BPE tokens in each chunk, while chunk_overlap defines the number of overlapping tokens between adjacent chunks. By modifying these parameters, you can fine-tune the granularity of the text chunks.\n",
        "\n",
        "One potential drawback of using TokenTextSplitter is that it may require additional computation when converting text to BPE tokens and back. If you need a faster and simpler text-splitting method, you might consider using CharacterTextSplitter, which directly splits the text based on character count, offering a more straightforward approach to text segmentation."
      ],
      "metadata": {
        "id": "H_qwqKoSy9KP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **RECAP:**\n",
        "\n",
        "Text splitters are essential for managing long text, improving language model processing efficiency, and enhancing vector store search results. Customizing text splitters involves selecting the splitting method and measuring chunk size.\n",
        "\n",
        "CharacterTextSplitter is an example that helps balance manageable pieces and semantic context preservation. Experimenting with different chunk sizes and overlaps tailor the results for specific use cases.\n",
        "\n",
        "RecursiveCharacterTextSplitter focuses on preserving semantic relationships while offering customizable chunk sizes and overlaps.\n",
        "\n",
        "NLTKTextSplitter utilizes the Natural Language Toolkit library for more accurate text segmentation. SpacyTextSplitter leverages the popular SpaCy library to split texts based on linguistic features. MarkdownTextSplitter is tailored for Markdown-formatted texts, ensuring content is split meaningfully according to the syntax. Lastly, TokenTextSplitter employs BPE tokens for splitting, offering a fine-grained approach to text segmentation.\n",
        "\n"
      ],
      "metadata": {
        "id": "VMnQCiy2y_XL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Conclusion**\n",
        "Selecting the appropriate text splitter depends on the specific requirements and nature of the text you are working with, ensuring optimal results for your text processing tasks.\n",
        "\n",
        "In the next lesson, we’ll learn more about how word embeddings work and how embedding models are used with indexers in LangChain."
      ],
      "metadata": {
        "id": "BZ60FX7lzEsB"
      }
    }
  ]
}