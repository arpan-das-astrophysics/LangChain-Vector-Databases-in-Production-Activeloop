{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arpan-das-astrophysics/LangChain-Vector-Databases-in-Production-Activeloop/blob/main/Using_Prompt_Templates.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Using Prompt Templates**"
      ],
      "metadata": {
        "id": "7ztGDPRB-xlY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Introduction**\n",
        "In the era of language models, the ability to perform a wide range of tasks is at our fingertips. These models operate on a straightforward principle: they accept a text input sequence and generate an output text sequence. The key factor in this process is the input text or prompt.\n",
        "\n",
        "Crafting suitable prompts is vital for anyone working with large language models, as poorly constructed prompts yield unsatisfactory outputs, while well-formulated prompts lead to powerful results. Recognizing the importance of prompts, the LangChain library has developed a comprehensive suite of objects tailored for them.\n",
        "\n",
        "This lesson delves into the nuances of PromptTemplates and how to employ them effectively. A PromptTemplate is a predefined structure or pattern used to construct effective and consistent prompts for large language models. It is a guideline to ensure the input text or prompt is properly formatted.\n",
        "\n",
        "Here's an example of using a PromptTemplate with a single dynamic input for a user query."
      ],
      "metadata": {
        "id": "TSTviH5h-zyG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "STjlSS-7kCoz",
        "outputId": "075f805a-b5bc-42a1-ebed-40eb0779fb40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m99.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q langchain==0.0.208 openai python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from getpass import getpass\n",
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XsV8LDNckFNY",
        "outputId": "fcbbeda1-6f9e-4715-dd81-ae3f9d895e93"
      },
      "execution_count": 3,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import LLMChain, PromptTemplate\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "llm = OpenAI(model_name=\"text-davinci-003\", temperature=0)\n",
        "\n",
        "template = \"\"\"Answer the question based on the context below. If the\n",
        "question cannot be answered using the information provided, answer\n",
        "with \"I don't know\".\n",
        "Context: Quantum computing is an emerging field that leverages quantum mechanics to solve complex problems faster than classical computers.\n",
        "...\n",
        "Question: {query}\n",
        "Answer: \"\"\"\n",
        "\n",
        "prompt_template = PromptTemplate(\n",
        "    input_variables=[\"query\"],\n",
        "    template=template\n",
        ")\n",
        "\n",
        "# Create the LLMChain for the prompt\n",
        "chain = LLMChain(llm=llm, prompt=prompt_template)\n",
        "\n",
        "# Set the query you want to ask\n",
        "input_data = {\"query\": \"What is the main advantage of quantum computing over classical computing?\"}\n",
        "\n",
        "# Run the LLMChain to get the AI-generated answer\n",
        "response = chain.run(input_data)\n",
        "\n",
        "print(\"Question:\", input_data[\"query\"])\n",
        "print(\"Answer:\", response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zfBouz-GkHpo",
        "outputId": "5c8ca2b6-dc2d-4a61-bb51-17a9c1bb4437"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: What is the main advantage of quantum computing over classical computing?\n",
            "Answer:  Quantum computing can solve complex problems faster than classical computers.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The template is a formatted string with a {query} placeholder that will be substituted with a real question when applied. To create a PromptTemplate object, two arguments are required:\n",
        "\n",
        "1. input_variables: A list of variable names in the template; in this case, it includes only the query.\n",
        "\n",
        "2. template: The template string containing formatted text and placeholders.\n",
        "After creating the PromptTemplate object, it can be used to produce prompts with specific questions by providing input data. The input data is a dictionary where the key corresponds to the variable name in the template. The resulting prompt can then be passed to a language model to generate answers.\n",
        "\n",
        "For more advanced usage, you can create a FewShotPromptTemplate with an ExampleSelector to select a subset of examples that will be most informative for the language model."
      ],
      "metadata": {
        "id": "z8LexZgc_jyW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import LLMChain, FewShotPromptTemplate, PromptTemplate\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "llm = OpenAI(model_name=\"text-davinci-003\", temperature=0)\n",
        "\n",
        "examples = [\n",
        "    {\"animal\": \"lion\", \"habitat\": \"savanna\"},\n",
        "    {\"animal\": \"polar bear\", \"habitat\": \"Arctic ice\"},\n",
        "    {\"animal\": \"elephant\", \"habitat\": \"African grasslands\"}\n",
        "]\n",
        "\n",
        "example_template = \"\"\"\n",
        "Animal: {animal}\n",
        "Habitat: {habitat}\n",
        "\"\"\"\n",
        "\n",
        "example_prompt = PromptTemplate(\n",
        "    input_variables=[\"animal\", \"habitat\"],\n",
        "    template=example_template\n",
        ")\n",
        "\n",
        "dynamic_prompt = FewShotPromptTemplate(\n",
        "    examples=examples,\n",
        "    example_prompt=example_prompt,\n",
        "    prefix=\"Identify the habitat of the given animal\",\n",
        "    suffix=\"Animal: {input}\\nHabitat:\",\n",
        "    input_variables=[\"input\"],\n",
        "    example_separator=\"\\n\\n\",\n",
        ")\n",
        "\n",
        "# Create the LLMChain for the dynamic_prompt\n",
        "chain = LLMChain(llm=llm, prompt=dynamic_prompt)\n",
        "\n",
        "# Run the LLMChain with input_data\n",
        "input_data = {\"input\": \"tiger\"}\n",
        "response = chain.run(input_data)\n",
        "\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4nUPXHskR39",
        "outputId": "a5b607c2-fcf5-4025-c810-547144600f9d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " tropical forests and mangrove swamps\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Additionally, we can also save our PromptTemplate to a file in our local filesystem in JSON or YAML format:"
      ],
      "metadata": {
        "id": "kKEgNoMvAGRS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template.save(\"awesome_prompt.json\")"
      ],
      "metadata": {
        "id": "4djEqXz2kuVw"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And load it back:"
      ],
      "metadata": {
        "id": "aRMah3EdAOXb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import load_prompt\n",
        "loaded_prompt = load_prompt(\"awesome_prompt.json\")"
      ],
      "metadata": {
        "id": "CDEcazXml-W_"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's explore more examples using different types of Prompt Templates. In the next example, we see how to use a few shot prompts to teach the LLM by providing examples to respond sarcastically to questions."
      ],
      "metadata": {
        "id": "0UICkSEvASQg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import LLMChain, FewShotPromptTemplate, PromptTemplate\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "llm = OpenAI(model_name=\"text-davinci-003\", temperature=0)\n",
        "\n",
        "examples = [\n",
        "    {\n",
        "        \"query\": \"How do I become a better programmer?\",\n",
        "        \"answer\": \"Try talking to a rubber duck; it works wonders.\"\n",
        "    }, {\n",
        "        \"query\": \"Why is the sky blue?\",\n",
        "        \"answer\": \"It's nature's way of preventing eye strain.\"\n",
        "    }\n",
        "]\n",
        "\n",
        "example_template = \"\"\"\n",
        "User: {query}\n",
        "AI: {answer}\n",
        "\"\"\"\n",
        "\n",
        "example_prompt = PromptTemplate(\n",
        "    input_variables=[\"query\", \"answer\"],\n",
        "    template=example_template\n",
        ")\n",
        "\n",
        "prefix = \"\"\"The following are excerpts from conversations with an AI\n",
        "assistant. The assistant is typically sarcastic and witty, producing\n",
        "creative and funny responses to users' questions. Here are some\n",
        "examples:\n",
        "\"\"\"\n",
        "\n",
        "suffix = \"\"\"\n",
        "User: {query}\n",
        "AI: \"\"\"\n",
        "\n",
        "few_shot_prompt_template = FewShotPromptTemplate(\n",
        "    examples=examples,\n",
        "    example_prompt=example_prompt,\n",
        "    prefix=prefix,\n",
        "    suffix=suffix,\n",
        "    input_variables=[\"query\"],\n",
        "    example_separator=\"\\n\\n\"\n",
        ")\n",
        "\n",
        "# Create the LLMChain for the few_shot_prompt_template\n",
        "chain = LLMChain(llm=llm, prompt=few_shot_prompt_template)\n",
        "\n",
        "# Run the LLMChain with input_data\n",
        "input_data = {\"query\": \"How can I learn quantum computing?\"}\n",
        "response = chain.run(input_data)\n",
        "\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0ufnnNyl_6q",
        "outputId": "f5ba99da-9d8a-488f-d55a-da3c003de689"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Start by studying Schrödinger's cat. That should give you a good foundation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `FewShotPromptTemplate` provided in the example demonstrates the power of dynamic prompts. Instead of using a static template, this approach incorporates examples of previous interactions, allowing the AI better to understand the context and style of the desired response.\n",
        "\n",
        "Dynamic prompts offer several advantages over static templates:\n",
        "\n",
        "* **Improved context understanding:** By providing examples, the AI can grasp the context and style of responses more effectively, enabling it to generate responses that are more in line with the desired output.\n",
        "* **Flexibility:** Dynamic prompts can be easily customized and adapted to specific use cases, allowing developers to experiment with different prompt structures and find the most effective format for their application.\n",
        "* Better results: As a result of the improved context understanding and flexibility, dynamic prompts often yield higher-quality outputs that better match user expectations.\n",
        "This allows us to take full advantage of the model's capabilities by providing examples and context that guide the AI toward generating more accurate, contextually relevant, and stylistically consistent responses.\n",
        "\n",
        "Prompt Templates also integrate well with other features in LangChain, like chains, and allow you to control the number of examples included based on query length. This helps in optimizing token usage and managing the balance between the number of examples and prompt size.\n",
        "\n",
        "To optimize the performance of few-shot learning, providing the model with as many relevant examples as possible without exceeding the maximum context window or causing excessive processing times is crucial. The dynamic inclusion or exclusion of examples allows us to strike a balance between providing sufficient context and maintaining efficiency in the model's operation:"
      ],
      "metadata": {
        "id": "63O0Ihw9Aiya"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "examples = [\n",
        "    {\n",
        "        \"query\": \"How do you feel today?\",\n",
        "        \"answer\": \"As an AI, I don't have feelings, but I've got jokes!\"\n",
        "    }, {\n",
        "        \"query\": \"What is the speed of light?\",\n",
        "        \"answer\": \"Fast enough to make a round trip around Earth 7.5 times in one second!\"\n",
        "    }, {\n",
        "        \"query\": \"What is a quantum computer?\",\n",
        "        \"answer\": \"A magical box that harnesses the power of subatomic particles to solve complex problems.\"\n",
        "    }, {\n",
        "        \"query\": \"Who invented the telephone?\",\n",
        "        \"answer\": \"Alexander Graham Bell, the original 'ringmaster'.\"\n",
        "    }, {\n",
        "        \"query\": \"What programming language is best for AI development?\",\n",
        "        \"answer\": \"Python, because it's the only snake that won't bite.\"\n",
        "    }, {\n",
        "        \"query\": \"What is the capital of France?\",\n",
        "        \"answer\": \"Paris, the city of love and baguettes.\"\n",
        "    }, {\n",
        "        \"query\": \"What is photosynthesis?\",\n",
        "        \"answer\": \"A plant's way of saying 'I'll turn this sunlight into food. You're welcome, Earth.'\"\n",
        "    }, {\n",
        "        \"query\": \"What is the tallest mountain on Earth?\",\n",
        "        \"answer\": \"Mount Everest, Earth's most impressive bump.\"\n",
        "    }, {\n",
        "        \"query\": \"What is the most abundant element in the universe?\",\n",
        "        \"answer\": \"Hydrogen, the basic building block of cosmic smoothies.\"\n",
        "    }, {\n",
        "        \"query\": \"What is the largest mammal on Earth?\",\n",
        "        \"answer\": \"The blue whale, the original heavyweight champion of the world.\"\n",
        "    }, {\n",
        "        \"query\": \"What is the fastest land animal?\",\n",
        "        \"answer\": \"The cheetah, the ultimate sprinter of the animal kingdom.\"\n",
        "    }, {\n",
        "        \"query\": \"What is the square root of 144?\",\n",
        "        \"answer\": \"12, the number of eggs you need for a really big omelette.\"\n",
        "    }, {\n",
        "        \"query\": \"What is the average temperature on Mars?\",\n",
        "        \"answer\": \"Cold enough to make a Martian wish for a sweater and a hot cocoa.\"\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "b0qYx377mGlR"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instead of utilizing the examples list of dictionaries directly, we implement a `LengthBasedExampleSelector` like this:"
      ],
      "metadata": {
        "id": "F7Rjh5_hBDXo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts.example_selector import LengthBasedExampleSelector\n",
        "\n",
        "example_selector = LengthBasedExampleSelector(\n",
        "    examples=examples,\n",
        "    example_prompt=example_prompt,\n",
        "    max_length=100\n",
        ")"
      ],
      "metadata": {
        "id": "kdui3d7Amxgr"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "By employing the `LengthBasedExampleSelector`, the code dynamically selects and includes examples based on their length, ensuring that the final prompt stays within the desired token limit. The selector is employed to initialize a `dynamic_prompt_template`:\n",
        "\n"
      ],
      "metadata": {
        "id": "S29NsZu-BK3t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dynamic_prompt_template = FewShotPromptTemplate(\n",
        "    example_selector=example_selector,\n",
        "    example_prompt=example_prompt,\n",
        "    prefix=prefix,\n",
        "    suffix=suffix,\n",
        "    input_variables=[\"query\"],\n",
        "    example_separator=\"\\n\"\n",
        ")"
      ],
      "metadata": {
        "id": "lyiUSvXHmye2"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, the `dynamic_prompt_template` utilizes the `example_selector` instead of a fixed list of examples. This allows the `FewShotPromptTemplate` to adjust the number of included examples based on the length of the input query. By doing so, it optimizes the use of the available context window and ensures that the language model receives an appropriate amount of contextual information.\n",
        "\n"
      ],
      "metadata": {
        "id": "lzWv3Ns3BR3w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import LLMChain, FewShotPromptTemplate, PromptTemplate\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.prompts.example_selector import LengthBasedExampleSelector\n",
        "\n",
        "llm = OpenAI(model_name=\"text-davinci-003\", temperature=0)\n",
        "\n",
        "# Existing example and prompt definitions, and dynamic_prompt_template initialization\n",
        "\n",
        "# Create the LLMChain for the dynamic_prompt_template\n",
        "chain = LLMChain(llm=llm, prompt=dynamic_prompt_template)\n",
        "\n",
        "# Run the LLMChain with input_data\n",
        "input_data = {\"query\": \"Who invented the telephone?\"}\n",
        "response = chain.run(input_data)\n",
        "\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BT34c4BkmzqD",
        "outputId": "130b542f-4aea-44ff-f993-55b40f041b16"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Alexander Graham Bell, the man who made it possible to talk to people from miles away!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Conclusion**\n",
        "Prompt Templates are essential for generating effective prompts for large language models, providing a structured and consistent format that maximizes accuracy and relevance. Integrating dynamic prompts enhances context understanding, flexibility, and results, making them a valuable asset for language model development."
      ],
      "metadata": {
        "id": "GkJBJ-xeBdVu"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ut0T3AsBm4lr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}