{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arpan-das-astrophysics/LangChain-Vector-Databases-in-Production-Activeloop/blob/main/Exploring_the_World_of_Language_Models_LLMs_vs_Chat_Models_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Exploring the World of Language Models: LLMs vs Chat Models**"
      ],
      "metadata": {
        "id": "fxT-HLEJ4uYd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Introduction**\n",
        "Large Language Models have made significant advancements in the field of Natural Language Processing (NLP), enabling AI systems to understand and generate human-like text. ChatGPT is a popular language model based on Transformers architecture, enabling it to understand long texts and figure out how words or ideas are connected. It's great at making predictions about language and relationships between words.\n",
        "\n",
        "LLMs and Chat Models are two types of models in LangChain, serving different purposes in natural language processing tasks. This lesson will examine the differences between LLMs and Chat Models, their unique use cases, and how they are implemented within LangChain."
      ],
      "metadata": {
        "id": "YjNhXS3d4wCt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Understanding LLMs and Chat Models**\n"
      ],
      "metadata": {
        "id": "HiI4_cKN4zdR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **LLMs**\n",
        "LLMs, such as GPT-3, Bloom, PaLM, and Aurora genAI, take a text string as input and return a text string as output. They are trained on language modeling tasks and can generate human-like text, perform complex reasoning, and even write code. LLMs are powerful and flexible, capable of generating text for a wide range of tasks. However, they can sometimes produce incorrect or nonsensical answers, and their API is less structured compared to Chat Models.\n",
        "\n",
        "Pre-training these models involves presenting large-scale corpora to them and allowing the network to predict the next word, which results in learning the relationships between words. This learning process enables LLMs to generate high-quality text, which can be applied to an array of applications, such as automatic form-filling and predictive text on smartphones.\n",
        "\n",
        "Most of these models are trained on general purpose training dataset, while others are trained on a mix of general and domain-specific data, such as Intel Aurora genAI, which is trained on general text, scientific texts, scientific data, and codes related to the domain. The goal of domain specific LLMs is to increase the performance on a particularly domain, while still being able to solve the majority of tasks that general LLM can manage.\n",
        "\n",
        "LLMs have the potential to infiltrate various aspects of human life, including the arts, sciences, and law. With continued development, LLMs will become increasingly integrated into our educational, personal, and professional lives, making them an essential technology to master."
      ],
      "metadata": {
        "id": "SVtC2_Ii43ca"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5BZkJZv3_wk",
        "outputId": "b79b8ca3-3531-4fe8-a409-5866041c2993"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.3/1.1 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q langchain==0.0.208 openai python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from getpass import getpass\n",
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cnb6iwlq8F0k",
        "outputId": "233330a0-48ff-49c2-e6e9-c0e44631c017"
      },
      "execution_count": 2,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain.prompts import PromptTemplate"
      ],
      "metadata": {
        "id": "LuDPxI-x4Twy"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI(model_name=\"text-davinci-003\", temperature=0)"
      ],
      "metadata": {
        "id": "ZdJlDzgK8Cel"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"product\"],\n",
        "    template=\"What is a good name for a company that makes {product}?\",\n",
        ")"
      ],
      "metadata": {
        "id": "rNk81HaE8DWF"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import LLMChain\n",
        "chain = LLMChain(llm=llm, prompt=prompt)"
      ],
      "metadata": {
        "id": "LPLkpoZV8rHc"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = chain.run(\"wireless headphones\")\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5VlzgOR_8xKF",
        "outputId": "689a6806-35ed-49af-99fb-c9004d40c984"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Wireless Audio Solutions\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, the input for the chain is the string \"wireless headphones\". The chain processes the input and generates a result based on the product name."
      ],
      "metadata": {
        "id": "C9kLdUN35d3G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Chat Models**\n",
        "Chat Models are the most popular models in LangChain, such as ChatGPT that can incorporate GPT-3 or GPT-4 at its core. They have gained significant attention due to their ability to learn from human feedback and their user-friendly chat interface.\n",
        "\n",
        "Chat Models, such as ChatGPT, take a list of messages as input and return an AIMessageCopy. They typically use LLMs as their underlying technology, but their APIs are more structured. Chat Models are designed to remember previous exchanges with the user in a session and use that context to generate more relevant responses. They also benefit from reinforcement learning from human feedback, which helps improve their responses. However, they may still have limitations in reasoning and require careful handling to avoid generating inappropriate content.\n",
        "\n",
        "API Differences in LangChain\n",
        "\n",
        "## **Chat Message Types**\n",
        "\n",
        "In LangChain, three main types of messages are used when interacting with chat models: SystemMessageCopy, HumanMessageCopy, and AIMessageCopy.\n",
        "\n",
        "**SystemMessage:** These messages provide initial instructions, context, or data for the AI model. They set the objectives the AI should follow and can help in controlling the AI's behavior. System messages are not user inputs but rather guidelines for the AI to operate within.\n",
        "\n",
        "**HumanMessage:** These messages come from the user and represent their input to the AI model. The AI model is expected to respond to these messages. In LangChain, you can customize the human prefix (e.g., \"User\") in the conversation summary to change how the human input is represented.\n",
        "\n",
        "**AIMessage:** These messages are sent from the AI's perspective as it interacts with the human user. They represent the AI's responses to human input. Like HumanMessage, you can also customize the AI prefix (e.g., \"AI Assistant\" or \"AI\") in the conversation summary to change how the AI's responses are represented.\n",
        "\n",
        "An example of using ChatOpenAI with a HumanMessage: In this section, we are trying to use the LangChain library to create a chatbot that can translate an English sentence into French. This particular use case goes beyond what we covered in the previous lesson. We'll be employing multiple message types to differentiate between users' queries and system instructions instead of relying on a single prompt. This approach will enhance the model's comprehension of the given requirements.\n",
        "\n",
        "First, we create a list of messages, starting with a SystemMessageCopy that sets the context for the chatbot, informing it that its role is to be a helpful assistant translating English to French. We then follow it with a HumanMessageCopy containing the user’s query, like an English sentence to be translated."
      ],
      "metadata": {
        "id": "AgLgnDzE8zbd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "openai.Model.list()"
      ],
      "metadata": {
        "id": "q0XeScay69BA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.schema import (\n",
        "    HumanMessage,\n",
        "    SystemMessage\n",
        ")\n",
        "\n",
        "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)"
      ],
      "metadata": {
        "id": "4AtRDt9i8x1i"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    SystemMessage(content=\"You are a helpful assistant that translates English to French.\"),\n",
        "    HumanMessage(content=\"Translate the following sentence: I love programming.\")\n",
        "]\n",
        "\n",
        "chat(messages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dv1OWi-f9EIl",
        "outputId": "d9b6c8e1-bea7-47b8-82eb-018c08d196d2"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"J'adore la programmation.\", additional_kwargs={}, example=False)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, we pass the list of messages to the chatbot using the chat()Copy function. The chatbot processes the input messages, considers the context provided by the system message, and then translates the given English sentence into French."
      ],
      "metadata": {
        "id": "vw89KIpY8NBZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SystemMessageCopy** represents the messages generated by the system that wants to use the model, which could include instructions, notifications, or error messages. These messages are not generated by the human user or the AI chatbot but are instead produced by the underlying system to provide context, guidance, or status updates.\n",
        "\n",
        "Using the generate method, you can also generate completions for multiple sets of messages. Each batch of messages can have its own SystemMessageCopy and will perform independently. The following code shows the first set of messages translate the sentences from English to French, while the second ones do the opposite."
      ],
      "metadata": {
        "id": "McGjw6HB8Psa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_messages = [\n",
        "    [\n",
        "        SystemMessage(content=\"You are a helpful assistant that translates English to French.\"),\n",
        "        HumanMessage(content=\"Translate the following sentence: I love programming.\")\n",
        "    ],\n",
        "    [\n",
        "        SystemMessage(content=\"You are a helpful assistant that translates French to English.\"),\n",
        "        HumanMessage(content=\"Translate the following sentence: J'aime la programmation.\")\n",
        "    ],\n",
        "]\n",
        "result = chat.generate(batch_messages)\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5V8LECK9EF9",
        "outputId": "a30bb886-2024-43a7-eef7-f43aa6c0969c"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LLMResult(generations=[[ChatGeneration(text=\"J'adore la programmation.\", generation_info=None, message=AIMessage(content=\"J'adore la programmation.\", additional_kwargs={}, example=False))], [ChatGeneration(text='I like programming.', generation_info=None, message=AIMessage(content='I like programming.', additional_kwargs={}, example=False))]], llm_output={'token_usage': {'prompt_tokens': 65, 'completion_tokens': 12, 'total_tokens': 77}, 'model_name': 'gpt-3.5-turbo'}, run=RunInfo(run_id=UUID('e6ac4a95-4e69-47c7-8257-7ab5362ad058')))"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As a comparison, here's what LLM and Chat Model APIs look like in LangChain."
      ],
      "metadata": {
        "id": "F4RzkSTEKR-n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm_input = \"Translate the following text from English to French: Hello, how are you?\"\n",
        "llm_output = chain(llm_input)"
      ],
      "metadata": {
        "id": "LdvIUD0FJOll"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"llm_output: \", llm_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p1X0gywLKSyP",
        "outputId": "e4974167-60cc-4108-80bc-b25eb8b8b83d"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "llm_output:  {'product': 'Translate the following text from English to French: Hello, how are you?', 'text': '\\n\\nBonjour, comment allez-vous?'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    SystemMessage(content=\"You are a helpful assistant that translates English to French.\"),\n",
        "    HumanMessage(content=\"Translate the following sentence: Hello, how are you?\")\n",
        "]\n",
        "chat_output = chat(messages)"
      ],
      "metadata": {
        "id": "-yJ-BV7yKcjj"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"chat_output: \", chat_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xvh33mPOKd8j",
        "outputId": "fcfe038f-016b-4077-ca8b-420b93f68079"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "chat_output:  content='Bonjour, comment ça va ?' additional_kwargs={} example=False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Conclusion**\n",
        "LLMs and Chat Models each have their advantages and disadvantages. LLMs are powerful and flexible, capable of generating text for a wide range of tasks. However, their API is less structured compared to Chat Models.\n",
        "\n",
        "On the other hand, Chat Models offer a more structured API and are better suited for conversational tasks. Also, they can remember previous exchanges with the user, making them more suitable for engaging in meaningful conversations. Additionally, they benefit from reinforcement learning from human feedback, which helps improve their responses. They still have some limitations in reasoning and may require careful handling to avoid hallucinations and generating inappropriate content.\n",
        "\n",
        "In the next lesson we’ll see how GPT-4 and ChatGPT can be used for context-aware chat applications via APIs."
      ],
      "metadata": {
        "id": "ju1Xur438py-"
      }
    }
  ]
}