{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arpan-das-astrophysics/LangChain-Vector-Databases-in-Production-Activeloop/blob/main/Getting_the_Best_of_Few_Shot_Prompts_and_Example_Selectors.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Getting the Best of Few Shot Prompts and Example Selectors**"
      ],
      "metadata": {
        "id": "lxtTsVfRRBaW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Introduction**\n",
        "In this lesson, we'll explore how few-shot prompts and example selectors can enhance the performance of language models in LangChain. Implementing Few-shot prompting and Example selection in LangChain can be achieved through various methods. We'll discuss three distinct approaches, examining their advantages and disadvantages to help you make the most of your language model."
      ],
      "metadata": {
        "id": "m2a6rQuLRElq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Alternating Human/AI messages\n",
        "In this strategy, few-shot prompting utilizes alternating human and AI messages. This technique can be especially beneficial for chat-oriented applications since the language model must comprehend the conversational context and provide appropriate responses.\n",
        "\n",
        "While this approach effectively handles conversation context and is easy to implement for chat-based applications, it lacks flexibility for other application types and is limited to chat-based models. However, we can use alternating human/AI messages to create a chat prompt that translates English into pirate language. The code snippet below demonstrates this approach."
      ],
      "metadata": {
        "id": "Fr3BpYsqRHr3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "esuXFO7tnYED",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "205d31c9-dbeb-4be5-9e0e-ba01dfc57561"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.3/1.1 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m526.0/526.0 kB\u001b[0m \u001b[31m51.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m77.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.7/135.7 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.8/79.8 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m72.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.7/72.7 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m135.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.7/132.7 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.8/79.8 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.8/52.8 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for deeplake (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q langchain==0.0.208 deeplake openai tiktoken python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from getpass import getpass\n",
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass()\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-4UEmugnaPa",
        "outputId": "a22b394c-df11-4e19-d52f-79bca7ef58fc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain import LLMChain\n",
        "from langchain.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    AIMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "\n",
        "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
        "\n",
        "template=\"You are a helpful assistant that translates english to pirate.\"\n",
        "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
        "example_human = HumanMessagePromptTemplate.from_template(\"Hi\")\n",
        "example_ai = AIMessagePromptTemplate.from_template(\"Argh me mateys\")\n",
        "human_template=\"{text}\"\n",
        "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, example_human, example_ai, human_message_prompt])\n",
        "chain = LLMChain(llm=chat, prompt=chat_prompt)\n",
        "chain.run(\"I love programming.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "FENrGCkonc8n",
        "outputId": "0ebd0859-94ad-4386-9ed4-223b9c2ed1d6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I be lovin' the art of code plunderin'.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Few-shot prompting**\n",
        "Few-shot prompting can lead to improved output quality because the model can learn the task better by observing the examples. However, the increased token usage may worsen the results if the examples are not well chosen or are misleading.\n",
        "\n",
        "This approach involves using the `FewShotPromptTemplate` class, which takes in a `PromptTemplate` and a list of a few shot examples. The class formats the prompt template with a few shot examples, which helps the language model generate a better response. We can streamline this process by utilizing LangChain's `FewShotPromptTemplate` to structure the approach:\n",
        "\n"
      ],
      "metadata": {
        "id": "uOVXNHeVSF7Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate, FewShotPromptTemplate\n",
        "\n",
        "# create our examples\n",
        "examples = [\n",
        "    {\n",
        "        \"query\": \"What's the weather like?\",\n",
        "        \"answer\": \"It's raining cats and dogs, better bring an umbrella!\"\n",
        "    }, {\n",
        "        \"query\": \"How old are you?\",\n",
        "        \"answer\": \"Age is just a number, but I'm timeless.\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# create an example template\n",
        "example_template = \"\"\"\n",
        "User: {query}\n",
        "AI: {answer}\n",
        "\"\"\"\n",
        "\n",
        "# create a prompt example from above template\n",
        "example_prompt = PromptTemplate(\n",
        "    input_variables=[\"query\", \"answer\"],\n",
        "    template=example_template\n",
        ")\n",
        "\n",
        "# now break our previous prompt into a prefix and suffix\n",
        "# the prefix is our instructions\n",
        "prefix = \"\"\"The following are excerpts from conversations with an AI\n",
        "assistant. The assistant is known for its humor and wit, providing\n",
        "entertaining and amusing responses to users' questions. Here are some\n",
        "examples:\n",
        "\"\"\"\n",
        "# and the suffix our user input and output indicator\n",
        "suffix = \"\"\"\n",
        "User: {query}\n",
        "AI: \"\"\"\n",
        "\n",
        "# now create the few-shot prompt template\n",
        "few_shot_prompt_template = FewShotPromptTemplate(\n",
        "    examples=examples,\n",
        "    example_prompt=example_prompt,\n",
        "    prefix=prefix,\n",
        "    suffix=suffix,\n",
        "    input_variables=[\"query\"],\n",
        "    example_separator=\"\\n\\n\"\n",
        ")"
      ],
      "metadata": {
        "id": "9lMQbTUentyr"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "fter creating a template, we pass the example and user query, we get the results."
      ],
      "metadata": {
        "id": "loczDOIkSi6e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chain = LLMChain(llm=chat, prompt=few_shot_prompt_template)\n",
        "chain.run(\"What's the secret to happiness?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Gn0CcQ7Hp7DO",
        "outputId": "cc2de494-b8e3-4615-9b41-b967ea77ae32"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'A lifetime supply of chocolate and a good sense of humor!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This method allows for better control over example formatting and is suitable for diverse applications, but it demands the manual creation of few-shot examples and can be less efficient with a large number of examples."
      ],
      "metadata": {
        "id": "Fl9p_221SmwE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Example selectors:**\n",
        "Example selectors can be used to provide a few-shot learning experience. The primary goal of few-shot learning is to learn a similarity function that maps the similarities between classes in the support and query sets. In this context, an example selector can be designed to choose a set of relevant examples that are representative of the desired output.\n",
        "\n",
        "The `ExampleSelector` is used to select a subset of examples that will be most informative for the language model. This helps in generating a prompt that is more likely to generate a good response. Also, the `LengthBasedExampleSelector` is useful when you're concerned about the length of the context window. It selects fewer examples for longer queries and more examples for shorter queries.\n",
        "\n",
        "Import the required classes:"
      ],
      "metadata": {
        "id": "tR0Ga9PwSofY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts.example_selector import LengthBasedExampleSelector\n",
        "from langchain.prompts import FewShotPromptTemplate, PromptTemplate"
      ],
      "metadata": {
        "id": "d-3xiPWmqGW3"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define your examples and the example_prompt"
      ],
      "metadata": {
        "id": "03oNhc5rS4pp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example = [\n",
        "    {\"word\": \"happy\", \"antonym\": \"sad\"},\n",
        "    {\"word\": \"tall\", \"antonym\": \"short\"},\n",
        "    {\"word\": \"energetic\", \"antonym\": \"lethargic\"},\n",
        "    {\"word\": \"sunny\", \"antonym\": \"gloomy\"},\n",
        "    {\"word\": \"windy\", \"antonym\": \"calm\"},\n",
        "]\n",
        "\n",
        "example_template = \"\"\"\n",
        "Word: {word}\n",
        "Antonym: {antonym}\n",
        "\"\"\"\n",
        "\n",
        "example_prompt = PromptTemplate(\n",
        "    input_variables=[\"word\", \"antonym\"],\n",
        "    template=example_template\n",
        ")"
      ],
      "metadata": {
        "id": "0BB49S7pqPup"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create an instance of LengthBasedExampleSelector"
      ],
      "metadata": {
        "id": "_tNcvq-YS_nZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_selector = LengthBasedExampleSelector(\n",
        "    examples=example,\n",
        "    example_prompt=example_prompt,\n",
        "    max_length=25,\n",
        ")"
      ],
      "metadata": {
        "id": "XddXd6iMqQ5y"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a `FewShotPromptTemplate`"
      ],
      "metadata": {
        "id": "v2ad23exTH57"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dynamic_prompt = FewShotPromptTemplate(\n",
        "    example_selector=example_selector,\n",
        "    example_prompt=example_prompt,\n",
        "    prefix=\"Give the antonym of every input\",\n",
        "    suffix=\"Word: {input}\\nAntonym:\",\n",
        "    input_variables=[\"input\"],\n",
        "    example_separator=\"\\n\\n\",\n",
        ")"
      ],
      "metadata": {
        "id": "ROPbx4VwqR0i"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate a prompt using the format method:"
      ],
      "metadata": {
        "id": "EvvaM0YlTPHs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(dynamic_prompt.format(input=\"big\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HoddCZIdtXAx",
        "outputId": "65890d7b-6793-4320-e0fc-38cea5578a5d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Give the antonym of every input\n",
            "\n",
            "\n",
            "Word: happy\n",
            "Antonym: sad\n",
            "\n",
            "\n",
            "\n",
            "Word: tall\n",
            "Antonym: short\n",
            "\n",
            "\n",
            "\n",
            "Word: energetic\n",
            "Antonym: lethargic\n",
            "\n",
            "\n",
            "\n",
            "Word: sunny\n",
            "Antonym: gloomy\n",
            "\n",
            "\n",
            "Word: big\n",
            "Antonym:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This method is effective for managing a large number of examples. It offers customization through various selectors, but it involves manual creation and selection of examples, which might not be ideal for every application type.\n",
        "\n",
        "Example of employing LangChain's `SemanticSimilarityExampleSelector` for selecting examples based on their semantic resemblance to the input. This illustration showcases the process of creating an ExampleSelector, generating a prompt using a few-shot approach:"
      ],
      "metadata": {
        "id": "ZKjM_GJ5TZLI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from getpass import getpass\n",
        "import os\n",
        "os.environ[\"ACTIVELOOP_TOKEN\"] = getpass()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bn9xO1z-UPAj",
        "outputId": "acf475eb-2442-4e7a-a4ed-0862e33ed041"
      },
      "execution_count": 12,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts.example_selector import SemanticSimilarityExampleSelector\n",
        "from langchain.vectorstores import DeepLake\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n",
        "\n",
        "# Create a PromptTemplate\n",
        "example_prompt = PromptTemplate(\n",
        "    input_variables=[\"input\", \"output\"],\n",
        "    template=\"Input: {input}\\nOutput: {output}\",\n",
        ")\n",
        "\n",
        "# Define some examples\n",
        "examples = [\n",
        "    {\"input\": \"0°C\", \"output\": \"32°F\"},\n",
        "    {\"input\": \"10°C\", \"output\": \"50°F\"},\n",
        "    {\"input\": \"20°C\", \"output\": \"68°F\"},\n",
        "    {\"input\": \"30°C\", \"output\": \"86°F\"},\n",
        "    {\"input\": \"40°C\", \"output\": \"104°F\"},\n",
        "]\n",
        "\n",
        "# create Deep Lake dataset\n",
        "my_activeloop_org_id = \"dasarpan007\" # TODO: use your organization id here\n",
        "my_activeloop_dataset_name = \"langchain_course_fewshot_selector\"\n",
        "dataset_path = f\"hub://{my_activeloop_org_id}/{my_activeloop_dataset_name}\"\n",
        "db = DeepLake(dataset_path=dataset_path)\n",
        "\n",
        "# Embedding function\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
        "\n",
        "# Instantiate SemanticSimilarityExampleSelector using the examples\n",
        "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
        "    examples, embeddings, db, k=1\n",
        ")\n",
        "\n",
        "# Create a FewShotPromptTemplate using the example_selector\n",
        "similar_prompt = FewShotPromptTemplate(\n",
        "    example_selector=example_selector,\n",
        "    example_prompt=example_prompt,\n",
        "    prefix=\"Convert the temperature from Celsius to Fahrenheit\",\n",
        "    suffix=\"Input: {temperature}\\nOutput:\",\n",
        "    input_variables=[\"temperature\"],\n",
        ")\n",
        "\n",
        "# Test the similar_prompt with different inputs\n",
        "print(similar_prompt.format(temperature=\"10°C\"))   # Test with an input\n",
        "print(similar_prompt.format(temperature=\"30°C\"))  # Test with another input\n",
        "\n",
        "# Add a new example to the SemanticSimilarityExampleSelector\n",
        "similar_prompt.example_selector.add_example({\"input\": \"50°C\", \"output\": \"122°F\"})\n",
        "print(similar_prompt.format(temperature=\"40°C\")) # Test with a new input after adding the example"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zl3SH4PPtlRe",
        "outputId": "0789df16-dbd6-47ac-eab7-6f31d38a103a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your Deep Lake dataset has been successfully created!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset(path='./deeplake/', tensors=['embedding', 'id', 'metadata', 'text'])\n",
            "\n",
            "  tensor      htype      shape     dtype  compression\n",
            "  -------    -------    -------   -------  ------- \n",
            " embedding  embedding  (5, 1536)  float32   None   \n",
            "    id        text      (5, 1)      str     None   \n",
            " metadata     json      (5, 1)      str     None   \n",
            "   text       text      (5, 1)      str     None   \n",
            "Convert the temperature from Celsius to Fahrenheit\n",
            "\n",
            "Input: 10°C\n",
            "Output: 50°F\n",
            "\n",
            "Input: 10°C\n",
            "Output:\n",
            "Convert the temperature from Celsius to Fahrenheit\n",
            "\n",
            "Input: 30°C\n",
            "Output: 86°F\n",
            "\n",
            "Input: 30°C\n",
            "Output:\n",
            "Dataset(path='./deeplake/', tensors=['embedding', 'id', 'metadata', 'text'])\n",
            "\n",
            "  tensor      htype      shape     dtype  compression\n",
            "  -------    -------    -------   -------  ------- \n",
            " embedding  embedding  (6, 1536)  float32   None   \n",
            "    id        text      (6, 1)      str     None   \n",
            " metadata     json      (6, 1)      str     None   \n",
            "   text       text      (6, 1)      str     None   \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r\r\r\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Convert the temperature from Celsius to Fahrenheit\n",
            "\n",
            "Input: 40°C\n",
            "Output: 104°F\n",
            "\n",
            "Input: 40°C\n",
            "Output:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Keep in mind that the `SemanticSimilarityExampleSelectoruses` the Deep Lake vector store and `OpenAIEmbeddings` to measure semantic similarity. It stores the samples on the database in the cloud, and retrieves similar samples.\n",
        "\n",
        "We created a `PromptTemplate` and defined several examples of temperature conversions. Next, we instantiated the `SemanticSimilarityExampleSelector` and created a `FewShotPromptTemplate` with the selector, example_prompt, and appropriate prefix and suffix.\n",
        "\n",
        "Using `SemanticSimilarityExampleSelector` and `FewShotPromptTemplate` , we enabled the creation of versatile prompts tailored to specific tasks or domains, like temperature conversion in this case. These tools provide a customizable and adaptable solution for generating prompts that can be used with language models to achieve a wide range of tasks."
      ],
      "metadata": {
        "id": "CE9o21d7VEUh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "To conclude, the utility of alternating human/AI interactions proves beneficial for chat-oriented applications, and the versatility offered by employing few-shot examples within a prompt template and selecting examples for the same extends its applicability across a broader spectrum of use cases. These methods necessitate a higher degree of manual intervention, as they require careful crafting and selection of apt examples. While these methods promise enhanced customization, they also underscore the importance of striking a balance between automation and manual input for optimal results."
      ],
      "metadata": {
        "id": "woRmjT3lVSn-"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vrSDR1KbVWN4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}